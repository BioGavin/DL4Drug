{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Protein Structure Data\n",
    "\n",
    "在定义数据集之前，我们首先需要定义我们想要对蛋白质进行的转换。我们考虑两种转换。 (1) 为了降低内存成本，截断过长的蛋白质序列是一种常见的做法。在TorchProtein中，我们可以通过指定max_length参数来定义蛋白质截断转换，以及通过random参数来确定是从随机残基还是从第一个残基开始截断。 (2) 此外，由于我们希望使用残基特征作为基于结构的模型的节点特征，我们还需要为蛋白质定义一个视图变换。在数据集构建过程中，我们可以将两种转换的组合作为参数传递。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2024-02-07T11:20:45.990004Z",
     "start_time": "2024-02-07T11:20:43.157097Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchdrug import transforms\n",
    "\n",
    "truncate_transform = transforms.TruncateProtein(max_length=350, random=False)\n",
    "protein_view_transform = transforms.ProteinView(view=\"residue\")\n",
    "transform = transforms.Compose([truncate_transform, protein_view_transform])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "为了在本教程中进行高效的计算，我们基于datasets.EnzymeCommission定义了一个小型蛋白质结构数据集EnzymeCommissionToy。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from torchdrug import datasets\n",
    "\n",
    "class EnzymeCommissionToy(datasets.EnzymeCommission):\n",
    "    url = \"https://miladeepgraphlearningproteindata.s3.us-east-2.amazonaws.com/data/EnzymeCommission.tar.gz\"\n",
    "    md5 = \"728e0625d1eb513fa9b7626e4d3bcf4d\"\n",
    "    processed_file = \"enzyme_commission_toy.pkl.gz\"\n",
    "    test_cutoffs = [0.3, 0.4, 0.5, 0.7, 0.95]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2024-02-07T11:20:45.999856Z",
     "start_time": "2024-02-07T11:20:45.989727Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "然后，我们根据这个子类实例化一个蛋白质结构数据集。在第一次实例化时，我们将保存一个名为enzyme_commission_toy.pkl.gz的压缩pickle文件，该文件将所有蛋白质结构数据存储到本地存储中。未来的实例化将直接加载这个pickle文件，因此速度会快得多。酶委员会数据集为每个蛋白质注解了538个二进制功能标签。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:20:47   Extracting /home/weibin/protein-datasets/EnzymeCommission.tar.gz to /home/weibin/protein-datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading /home/weibin/protein-datasets/EnzymeCommission/enzyme_commission_toy.pkl.gz: 100%|██████████| 1151/1151 [00:07<00:00, 157.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration of first instantiation:  8.501194953918457\n",
      "19:20:55   Extracting /home/weibin/protein-datasets/EnzymeCommission.tar.gz to /home/weibin/protein-datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading /home/weibin/protein-datasets/EnzymeCommission/enzyme_commission_toy.pkl.gz: 100%|██████████| 1151/1151 [00:07<00:00, 146.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration of second instantiation:  8.833615779876709\n",
      "Shape of function labels for a protein:  torch.Size([538])\n",
      "train samples: 959, valid samples: 97, test samples: 95\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "dataset = EnzymeCommissionToy(\"~/protein-datasets/\", transform=transform, atom_feature=None,\n",
    "                            bond_feature=None)\n",
    "end_time = time.time()\n",
    "print(\"Duration of first instantiation: \", end_time - start_time)\n",
    "\n",
    "start_time = time.time()\n",
    "dataset = EnzymeCommissionToy(\"~/protein-datasets/\", transform=transform, atom_feature=None,\n",
    "                            bond_feature=None)\n",
    "end_time = time.time()\n",
    "print(\"Duration of second instantiation: \", end_time - start_time)\n",
    "\n",
    "train_set, valid_set, test_set = dataset.split()\n",
    "print(\"Shape of function labels for a protein: \", dataset[0][\"targets\"].shape)\n",
    "print(\"train samples: %d, valid samples: %d, test samples: %d\" % (len(train_set), len(valid_set), len(test_set)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-07T11:21:03.629386Z",
     "start_time": "2024-02-07T11:20:45.990219Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dynamic Graph Construction\n",
    "\n",
    "TorchProtein使用RDKit来构建蛋白质图。由RDKit构建的蛋白质图只包含四种类型的键边（即，单键、双键、三键或芳香键）。给定数据集的第一个样本，我们选择出第一个和第二个残基的原子，并可视化它们之间的化学键。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from torchdrug import data\n",
    "\n",
    "protein = dataset[0][\"graph\"]\n",
    "is_first_two = (protein.residue_number == 1) | (protein.residue_number == 2)\n",
    "first_two = protein.residue_mask(is_first_two, compact=True)\n",
    "first_two.visualize()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-07T11:21:04.107892Z",
     "start_time": "2024-02-07T11:21:03.629269Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "然而，仅仅使用键边，无法充分利用蛋白质的丰富结构信息。在接下来的步骤中，我们将尝试动态重构蛋白质图，以更好地表示蛋白质的结构。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1: Construct Residue-level Graph\n",
    "\n",
    "我们以数据集的第一个样本为例。对于这个样本，原始的原子级图包含2956个节点，这对于大多数基于结构的模型（如GNNs）来说是无法承受的。因此，在第一步中，我们希望通过仅保留Alpha碳的节点来减小图的大小，从而构建一个残基级的图。我们通过layers.GraphConstruction和layers.geometry.AlphaCarbonNode模块来实现这个目标。layers.geometry.AlphaCarbonNode将丢弃那些没有Alpha碳的无效残基。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph before:  PackedProtein(batch_size=1, num_atoms=[2639], num_bonds=[5368], num_residues=[350])\n",
      "Graph after:  PackedProtein(batch_size=1, num_atoms=[350], num_bonds=[0], num_residues=[350])\n"
     ]
    }
   ],
   "source": [
    "from torchdrug import layers\n",
    "from torchdrug.layers import geometry\n",
    "\n",
    "graph_construction_model = layers.GraphConstruction(node_layers=[geometry.AlphaCarbonNode()])\n",
    "\n",
    "_protein = data.Protein.pack([protein])\n",
    "protein_ = graph_construction_model(_protein)\n",
    "print(\"Graph before: \", _protein)\n",
    "print(\"Graph after: \", protein_)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-07T11:21:04.160097Z",
     "start_time": "2024-02-07T11:21:04.105219Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "注意: 衍生的残基级图不包含边，因为没有两个Alpha碳通过化学键相连。因此，在接下来的步骤中，我们将尝试向这个残基级图中添加不同类型的边。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2: Add Spatial Edges\n",
    "\n",
    "在没有边的残基级图上，我们首先考虑在残基之间添加空间边，其距离在一个空间距离阈值内。此外，我们还会删除在蛋白质序列中彼此靠近的残基之间的空间边，因为这些边与折叠结构的关联较小。我们通过layers.geometry.SpatialEdge模块来实现这一目标。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph before:  PackedProtein(batch_size=1, num_atoms=[2639], num_bonds=[5368], num_residues=[350])\n",
      "Graph after:  PackedProtein(batch_size=1, num_atoms=[350], num_bonds=[4177], num_residues=[350])\n",
      "Average degree:  23.868572235107422\n",
      "Maximum degree:  51.0\n",
      "Minimum degree:  0.0\n",
      "Number of zero-degree nodes:  5\n"
     ]
    }
   ],
   "source": [
    "graph_construction_model = layers.GraphConstruction(node_layers=[geometry.AlphaCarbonNode()],\n",
    "                                                    edge_layers=[geometry.SpatialEdge(radius=10.0, min_distance=5)])\n",
    "\n",
    "_protein = data.Protein.pack([protein])\n",
    "protein_ = graph_construction_model(_protein)\n",
    "print(\"Graph before: \", _protein)\n",
    "print(\"Graph after: \", protein_)\n",
    "\n",
    "degree = protein_.degree_in + protein_.degree_out\n",
    "print(\"Average degree: \", degree.mean().item())\n",
    "print(\"Maximum degree: \", degree.max().item())\n",
    "print(\"Minimum degree: \", degree.min().item())\n",
    "print(\"Number of zero-degree nodes: \", (degree == 0).sum().item())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-07T11:21:04.308348Z",
     "start_time": "2024-02-07T11:21:04.160458Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "注意: 仅使用空间边，会有五个节点与图中的任何节点都没有连接，这会阻止在GNN模型中通过这些节点传递信息。在下一步中，我们将尝试通过利用KNN边来解决这个问题。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3: Add KNN Edges\n",
    "\n",
    "基于上述的残基级图，我们进一步考虑添加KNN边，其中每个节点将与其K个最近邻节点相连。同时，我们将删除蛋白质序列中彼此靠近的残基之间的KNN边。我们通过使用layers.geometry.KNNEdge模块来实现这一目标。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph before:  PackedProtein(batch_size=1, num_atoms=[2639], num_bonds=[5368], num_residues=[350])\n",
      "Graph after:  PackedProtein(batch_size=1, num_atoms=[350], num_bonds=[5532], num_residues=[350])\n",
      "Average degree:  tensor(31.6114)\n",
      "Maximum degree:  tensor(66.)\n",
      "Minimum degree:  tensor(2.)\n",
      "Number of zero-degree nodes:  tensor(0)\n"
     ]
    }
   ],
   "source": [
    "graph_construction_model = layers.GraphConstruction(node_layers=[geometry.AlphaCarbonNode()],\n",
    "                                                    edge_layers=[geometry.SpatialEdge(radius=10.0, min_distance=5),\n",
    "                                                                 geometry.KNNEdge(k=10, min_distance=5)])\n",
    "\n",
    "_protein = data.Protein.pack([protein])\n",
    "protein_ = graph_construction_model(_protein)\n",
    "print(\"Graph before: \", _protein)\n",
    "print(\"Graph after: \", protein_)\n",
    "\n",
    "degree = protein_.degree_in + protein_.degree_out\n",
    "print(\"Average degree: \", degree.mean())\n",
    "print(\"Maximum degree: \", degree.max())\n",
    "print(\"Minimum degree: \", degree.min())\n",
    "print(\"Number of zero-degree nodes: \", (degree == 0).sum())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-07T11:21:04.356295Z",
     "start_time": "2024-02-07T11:21:04.309929Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "注意: 在这种情况下，不再存在零度节点。然而，无论是空间边还是KNN边都忽略了蛋白质序列中彼此靠近的残基之间的边。为了补充这种缺失的信息，接下来我们考虑添加顺序边。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 4: Add Sequential Edges\n",
    "\n",
    "基于上述的残基级图，我们进一步考虑添加顺序边，其中在一个顺序距离阈值内的两个残基将相互连接起来。我们通过使用layers.geometry.SequentialEdge模块来实现这一目标。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph before:  PackedProtein(batch_size=1, num_atoms=[2639], num_bonds=[5368], num_residues=[350])\n",
      "Graph after:  PackedProtein(batch_size=1, num_atoms=[350], num_bonds=[7276], num_residues=[350])\n",
      "Average degree:  tensor(41.5771)\n",
      "Maximum degree:  tensor(76.)\n",
      "Minimum degree:  tensor(12.)\n",
      "Number of zero-degree nodes:  tensor(0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/weibin/miniconda3/envs/torchdrug/lib/python3.8/site-packages/torchdrug/layers/functional/functional.py:474: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  index1 = local_index // local_inner_size + offset1\n"
     ]
    }
   ],
   "source": [
    "graph_construction_model = layers.GraphConstruction(node_layers=[geometry.AlphaCarbonNode()],\n",
    "                                                    edge_layers=[geometry.SpatialEdge(radius=10.0, min_distance=5),\n",
    "                                                                 geometry.KNNEdge(k=10, min_distance=5),\n",
    "                                                                 geometry.SequentialEdge(max_distance=2)])\n",
    "\n",
    "_protein = data.Protein.pack([protein])\n",
    "protein_ = graph_construction_model(_protein)\n",
    "print(\"Graph before: \", _protein)\n",
    "print(\"Graph after: \", protein_)\n",
    "\n",
    "degree = protein_.degree_in + protein_.degree_out\n",
    "print(\"Average degree: \", degree.mean())\n",
    "print(\"Maximum degree: \", degree.max())\n",
    "print(\"Minimum degree: \", degree.min())\n",
    "print(\"Number of zero-degree nodes: \", (degree == 0).sum())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-07T11:21:04.444515Z",
     "start_time": "2024-02-07T11:21:04.354715Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Overview: Represent Protein Structure as Relational Graph\n",
    "\n",
    "在进行这样的图构建之后，我们将蛋白质结构表示为一个残基级别的关系图。将空间边和KNN边视为两种类型的边，并将五种不同的顺序距离（即-2，-1，0，1和2）的顺序边视为五种边类型，我们得到了一个具有7种不同边类型的关系图。每条边都与一个40维边特征相关联，该特征是其两个端点的独热残基特征的连接。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ILE -> VAL: type 1\n",
      "TRP -> GLU: type 1\n",
      "LEU -> GLU: type 1\n",
      "VAL -> GLU: type 1\n",
      "ARG -> ASP: type 1\n"
     ]
    }
   ],
   "source": [
    "nodes_in, nodes_out, edges_type = protein_.edge_list.t()\n",
    "residue_ids = protein_.residue_type.tolist()\n",
    "for node_in, node_out, edge_type in zip(nodes_in.tolist()[:5], nodes_out.tolist()[:5], edges_type.tolist()[:5]):\n",
    "    print(\"%s -> %s: type %d\" % (data.Protein.id2residue[residue_ids[node_in]],\n",
    "                                 data.Protein.id2residue[residue_ids[node_out]], edge_type))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-07T11:21:04.456557Z",
     "start_time": "2024-02-07T11:21:04.446863Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Protein Structure Representation Model\n",
    "\n",
    "TorchProtein定义了多种GNN模型，可以作为蛋白质结构编码器。在本教程中，我们将研究优秀的几何感知关系图神经网络（GearNet）及其在我们的小型基准上进行边消息传递扩展的模型（GearNet-Edge）。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## GearNet\n",
    "\n",
    "GearNet是专门设计用于编码上述定义的残基级别关系图的模型，其关键组成部分是不同残基之间的关系消息传递。在TorchProtein中，我们可以使用models.GearNet来定义一个GearNet模型。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from torchdrug import models\n",
    "\n",
    "gearnet = models.GearNet(input_dim=21, hidden_dims=[512, 512, 512], num_relation=7,\n",
    "                         batch_norm=True, concat_hidden=True, short_cut=True, readout=\"sum\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-07T11:21:04.888331Z",
     "start_time": "2024-02-07T11:21:04.453845Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## GearNet-Edge\n",
    "\n",
    "GearNet-Edge通过添加边级消息传递来扩展原始的GearNet模型。具体而言，GearNet-Edge构建了线图，其节点是原始图的边，并且它连接了原始图中相邻的边。在此基础上，通过对线图进行关系消息传递来实现边级消息传递。在TorchProtein中，我们可以使用models.GearNet来定义一个GearNet-Edge模型。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "gearnet_edge = models.GearNet(input_dim=21, hidden_dims=[512, 512, 512],\n",
    "                              num_relation=7, edge_input_dim=59, num_angle_bin=8,\n",
    "                              batch_norm=True, concat_hidden=True, short_cut=True, readout=\"sum\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-07T11:21:04.999112Z",
     "start_time": "2024-02-07T11:21:04.892406Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Structure-based Protein Function Prediction\n",
    "\n",
    "在这个部分，我们将解决小型酶委员会数据集上的蛋白质功能术语预测任务。我们使用GearNet和GearNet-Edge来解决这个任务，并比较它们的性能。\n",
    "\n",
    "注意: 这个任务旨在预测一个蛋白质是否具有几个特定的功能，其中每个功能可以用一个二进制标签来表示。因此，我们将这个任务形式化为多个二分类任务，并通过多任务学习的方式来共同解决它们。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Protein Function Prediction with GearNet\n",
    "\n",
    "我们首先将GearNet模型包装到tasks.MultipleBinaryClassification模块中，该模块可以同时执行所有考虑的二分类任务。在GearNet之上附加了一个MLP预测头，用于生成所有任务的预测结果。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from torchdrug import tasks\n",
    "\n",
    "graph_construction_model = layers.GraphConstruction(node_layers=[geometry.AlphaCarbonNode()],\n",
    "                                                    edge_layers=[geometry.SpatialEdge(radius=10.0, min_distance=5),\n",
    "                                                                 geometry.KNNEdge(k=10, min_distance=5),\n",
    "                                                                 geometry.SequentialEdge(max_distance=2)],\n",
    "                                                    edge_feature=\"gearnet\")\n",
    "\n",
    "task = tasks.MultipleBinaryClassification(gearnet, graph_construction_model=graph_construction_model, num_mlp_layer=3,\n",
    "                                          task=[_ for _ in range(len(dataset.tasks))], criterion=\"bce\", metric=[\"auprc@micro\", \"f1_max\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-07T11:21:05.135210Z",
     "start_time": "2024-02-07T11:21:05.001386Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "现在我们可以训练这个模型了。我们为这个模型设置一个优化器，并将所有内容放在一个Engine实例中。在这个任务上，训练模型进行10个epochs大约需要2分钟的时间。最后，我们对验证集进行评估。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:21:05   Preprocess training set\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 5\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchdrug\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m core\n\u001B[1;32m      4\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mAdam(task\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-4\u001B[39m)\n\u001B[0;32m----> 5\u001B[0m solver \u001B[38;5;241m=\u001B[39m \u001B[43mcore\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mEngine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_set\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalid_set\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_set\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m                     \u001B[49m\u001B[43mgpus\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      7\u001B[0m solver\u001B[38;5;241m.\u001B[39mtrain(num_epoch\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m)\n\u001B[1;32m      8\u001B[0m solver\u001B[38;5;241m.\u001B[39mevaluate(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalid\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/miniconda3/envs/torchdrug/lib/python3.8/site-packages/decorator.py:232\u001B[0m, in \u001B[0;36mdecorate.<locals>.fun\u001B[0;34m(*args, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m kwsyntax:\n\u001B[1;32m    231\u001B[0m     args, kw \u001B[38;5;241m=\u001B[39m fix(args, kw, sig)\n\u001B[0;32m--> 232\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcaller\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mextras\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/torchdrug/lib/python3.8/site-packages/torchdrug/core/core.py:288\u001B[0m, in \u001B[0;36m_Configurable.__new__.<locals>.wrapper\u001B[0;34m(init, self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    286\u001B[0m     config\u001B[38;5;241m.\u001B[39mpop(k)\n\u001B[1;32m    287\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_config \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(config)\n\u001B[0;32m--> 288\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minit\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/torchdrug/lib/python3.8/site-packages/torchdrug/core/engine.py:101\u001B[0m, in \u001B[0;36mEngine.__init__\u001B[0;34m(self, task, train_set, valid_set, test_set, optimizer, scheduler, gpus, batch_size, gradient_interval, num_worker, logger, log_interval)\u001B[0m\n\u001B[1;32m     99\u001B[0m     task \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mSyncBatchNorm\u001B[38;5;241m.\u001B[39mconvert_sync_batchnorm(task)\n\u001B[1;32m    100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice\u001B[38;5;241m.\u001B[39mtype \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 101\u001B[0m     task \u001B[38;5;241m=\u001B[39m \u001B[43mtask\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    103\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;241m=\u001B[39m task\n\u001B[1;32m    104\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_set \u001B[38;5;241m=\u001B[39m train_set\n",
      "File \u001B[0;32m~/miniconda3/envs/torchdrug/lib/python3.8/site-packages/torch/nn/modules/module.py:680\u001B[0m, in \u001B[0;36mModule.cuda\u001B[0;34m(self, device)\u001B[0m\n\u001B[1;32m    663\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcuda\u001B[39m(\u001B[38;5;28mself\u001B[39m: T, device: Optional[Union[\u001B[38;5;28mint\u001B[39m, device]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m T:\n\u001B[1;32m    664\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001B[39;00m\n\u001B[1;32m    665\u001B[0m \n\u001B[1;32m    666\u001B[0m \u001B[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    678\u001B[0m \u001B[38;5;124;03m        Module: self\u001B[39;00m\n\u001B[1;32m    679\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 680\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/torchdrug/lib/python3.8/site-packages/torch/nn/modules/module.py:570\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn)\u001B[0m\n\u001B[1;32m    568\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_apply\u001B[39m(\u001B[38;5;28mself\u001B[39m, fn):\n\u001B[1;32m    569\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 570\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    572\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[1;32m    573\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[1;32m    574\u001B[0m             \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[1;32m    575\u001B[0m             \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    580\u001B[0m             \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[1;32m    581\u001B[0m             \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/torchdrug/lib/python3.8/site-packages/torch/nn/modules/module.py:570\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn)\u001B[0m\n\u001B[1;32m    568\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_apply\u001B[39m(\u001B[38;5;28mself\u001B[39m, fn):\n\u001B[1;32m    569\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 570\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    572\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[1;32m    573\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[1;32m    574\u001B[0m             \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[1;32m    575\u001B[0m             \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    580\u001B[0m             \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[1;32m    581\u001B[0m             \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "    \u001B[0;31m[... skipping similar frames: Module._apply at line 570 (1 times)]\u001B[0m\n",
      "File \u001B[0;32m~/miniconda3/envs/torchdrug/lib/python3.8/site-packages/torch/nn/modules/module.py:570\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn)\u001B[0m\n\u001B[1;32m    568\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_apply\u001B[39m(\u001B[38;5;28mself\u001B[39m, fn):\n\u001B[1;32m    569\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 570\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    572\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[1;32m    573\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[1;32m    574\u001B[0m             \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[1;32m    575\u001B[0m             \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    580\u001B[0m             \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[1;32m    581\u001B[0m             \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/torchdrug/lib/python3.8/site-packages/torch/nn/modules/module.py:593\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn)\u001B[0m\n\u001B[1;32m    589\u001B[0m \u001B[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001B[39;00m\n\u001B[1;32m    590\u001B[0m \u001B[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001B[39;00m\n\u001B[1;32m    591\u001B[0m \u001B[38;5;66;03m# `with torch.no_grad():`\u001B[39;00m\n\u001B[1;32m    592\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m--> 593\u001B[0m     param_applied \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparam\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    594\u001B[0m should_use_set_data \u001B[38;5;241m=\u001B[39m compute_should_use_set_data(param, param_applied)\n\u001B[1;32m    595\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m should_use_set_data:\n",
      "File \u001B[0;32m~/miniconda3/envs/torchdrug/lib/python3.8/site-packages/torch/nn/modules/module.py:680\u001B[0m, in \u001B[0;36mModule.cuda.<locals>.<lambda>\u001B[0;34m(t)\u001B[0m\n\u001B[1;32m    663\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcuda\u001B[39m(\u001B[38;5;28mself\u001B[39m: T, device: Optional[Union[\u001B[38;5;28mint\u001B[39m, device]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m T:\n\u001B[1;32m    664\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001B[39;00m\n\u001B[1;32m    665\u001B[0m \n\u001B[1;32m    666\u001B[0m \u001B[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    678\u001B[0m \u001B[38;5;124;03m        Module: self\u001B[39;00m\n\u001B[1;32m    679\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 680\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_apply(\u001B[38;5;28;01mlambda\u001B[39;00m t: \u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchdrug import core\n",
    "\n",
    "optimizer = torch.optim.Adam(task.parameters(), lr=1e-4)\n",
    "solver = core.Engine(task, train_set, valid_set, test_set, optimizer,\n",
    "                     gpus=[0], batch_size=4)\n",
    "solver.train(num_epoch=10)\n",
    "solver.evaluate(\"valid\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-07T11:21:10.695595Z",
     "start_time": "2024-02-07T11:21:05.128573Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Protein Function Prediction with GearNet-Edge\n",
    "\n",
    "接下来，我们将GearNet-Edge模型包装到tasks.MultipleBinaryClassification模块中，该模块可以同时执行所有考虑的二分类任务。在GearNet-Edge之上附加了一个MLP预测头，用于生成所有任务的预测结果。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "graph_construction_model = layers.GraphConstruction(node_layers=[geometry.AlphaCarbonNode()],\n",
    "                                                    edge_layers=[geometry.SpatialEdge(radius=10.0, min_distance=5),\n",
    "                                                                 geometry.KNNEdge(k=10, min_distance=5),\n",
    "                                                                 geometry.SequentialEdge(max_distance=2)],\n",
    "                                                    edge_feature=\"gearnet\")\n",
    "\n",
    "task = tasks.MultipleBinaryClassification(gearnet_edge, graph_construction_model=graph_construction_model, num_mlp_layer=3,\n",
    "                                          task=[_ for _ in range(len(dataset.tasks))], criterion=\"bce\", metric=[\"auprc@micro\", \"f1_max\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-07T11:23:51.459414Z",
     "start_time": "2024-02-07T11:23:51.368970Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们对模型进行10个epochs的训练，大约需要8分钟的时间，最后在验证集上进行评估。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:23:54   Preprocess training set\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mAdam(task\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-4\u001B[39m)\n\u001B[0;32m----> 2\u001B[0m solver \u001B[38;5;241m=\u001B[39m \u001B[43mcore\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mEngine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_set\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalid_set\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_set\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m                     \u001B[49m\u001B[43mgpus\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m solver\u001B[38;5;241m.\u001B[39mtrain(num_epoch\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m)\n\u001B[1;32m      5\u001B[0m solver\u001B[38;5;241m.\u001B[39mevaluate(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalid\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/miniconda3/envs/torchdrug/lib/python3.8/site-packages/decorator.py:232\u001B[0m, in \u001B[0;36mdecorate.<locals>.fun\u001B[0;34m(*args, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m kwsyntax:\n\u001B[1;32m    231\u001B[0m     args, kw \u001B[38;5;241m=\u001B[39m fix(args, kw, sig)\n\u001B[0;32m--> 232\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcaller\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mextras\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/torchdrug/lib/python3.8/site-packages/torchdrug/core/core.py:288\u001B[0m, in \u001B[0;36m_Configurable.__new__.<locals>.wrapper\u001B[0;34m(init, self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    286\u001B[0m     config\u001B[38;5;241m.\u001B[39mpop(k)\n\u001B[1;32m    287\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_config \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(config)\n\u001B[0;32m--> 288\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minit\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/torchdrug/lib/python3.8/site-packages/torchdrug/core/engine.py:101\u001B[0m, in \u001B[0;36mEngine.__init__\u001B[0;34m(self, task, train_set, valid_set, test_set, optimizer, scheduler, gpus, batch_size, gradient_interval, num_worker, logger, log_interval)\u001B[0m\n\u001B[1;32m     99\u001B[0m     task \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mSyncBatchNorm\u001B[38;5;241m.\u001B[39mconvert_sync_batchnorm(task)\n\u001B[1;32m    100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice\u001B[38;5;241m.\u001B[39mtype \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 101\u001B[0m     task \u001B[38;5;241m=\u001B[39m \u001B[43mtask\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    103\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;241m=\u001B[39m task\n\u001B[1;32m    104\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_set \u001B[38;5;241m=\u001B[39m train_set\n",
      "File \u001B[0;32m~/miniconda3/envs/torchdrug/lib/python3.8/site-packages/torch/nn/modules/module.py:680\u001B[0m, in \u001B[0;36mModule.cuda\u001B[0;34m(self, device)\u001B[0m\n\u001B[1;32m    663\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcuda\u001B[39m(\u001B[38;5;28mself\u001B[39m: T, device: Optional[Union[\u001B[38;5;28mint\u001B[39m, device]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m T:\n\u001B[1;32m    664\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001B[39;00m\n\u001B[1;32m    665\u001B[0m \n\u001B[1;32m    666\u001B[0m \u001B[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    678\u001B[0m \u001B[38;5;124;03m        Module: self\u001B[39;00m\n\u001B[1;32m    679\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 680\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/torchdrug/lib/python3.8/site-packages/torch/nn/modules/module.py:570\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn)\u001B[0m\n\u001B[1;32m    568\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_apply\u001B[39m(\u001B[38;5;28mself\u001B[39m, fn):\n\u001B[1;32m    569\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 570\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    572\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[1;32m    573\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[1;32m    574\u001B[0m             \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[1;32m    575\u001B[0m             \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    580\u001B[0m             \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[1;32m    581\u001B[0m             \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/torchdrug/lib/python3.8/site-packages/torch/nn/modules/module.py:570\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn)\u001B[0m\n\u001B[1;32m    568\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_apply\u001B[39m(\u001B[38;5;28mself\u001B[39m, fn):\n\u001B[1;32m    569\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 570\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    572\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[1;32m    573\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[1;32m    574\u001B[0m             \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[1;32m    575\u001B[0m             \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    580\u001B[0m             \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[1;32m    581\u001B[0m             \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "    \u001B[0;31m[... skipping similar frames: Module._apply at line 570 (1 times)]\u001B[0m\n",
      "File \u001B[0;32m~/miniconda3/envs/torchdrug/lib/python3.8/site-packages/torch/nn/modules/module.py:570\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn)\u001B[0m\n\u001B[1;32m    568\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_apply\u001B[39m(\u001B[38;5;28mself\u001B[39m, fn):\n\u001B[1;32m    569\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 570\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    572\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[1;32m    573\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[1;32m    574\u001B[0m             \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[1;32m    575\u001B[0m             \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    580\u001B[0m             \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[1;32m    581\u001B[0m             \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/torchdrug/lib/python3.8/site-packages/torch/nn/modules/module.py:593\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn)\u001B[0m\n\u001B[1;32m    589\u001B[0m \u001B[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001B[39;00m\n\u001B[1;32m    590\u001B[0m \u001B[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001B[39;00m\n\u001B[1;32m    591\u001B[0m \u001B[38;5;66;03m# `with torch.no_grad():`\u001B[39;00m\n\u001B[1;32m    592\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m--> 593\u001B[0m     param_applied \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparam\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    594\u001B[0m should_use_set_data \u001B[38;5;241m=\u001B[39m compute_should_use_set_data(param, param_applied)\n\u001B[1;32m    595\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m should_use_set_data:\n",
      "File \u001B[0;32m~/miniconda3/envs/torchdrug/lib/python3.8/site-packages/torch/nn/modules/module.py:680\u001B[0m, in \u001B[0;36mModule.cuda.<locals>.<lambda>\u001B[0;34m(t)\u001B[0m\n\u001B[1;32m    663\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcuda\u001B[39m(\u001B[38;5;28mself\u001B[39m: T, device: Optional[Union[\u001B[38;5;28mint\u001B[39m, device]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m T:\n\u001B[1;32m    664\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001B[39;00m\n\u001B[1;32m    665\u001B[0m \n\u001B[1;32m    666\u001B[0m \u001B[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    678\u001B[0m \u001B[38;5;124;03m        Module: self\u001B[39;00m\n\u001B[1;32m    679\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 680\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_apply(\u001B[38;5;28;01mlambda\u001B[39;00m t: \u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(task.parameters(), lr=1e-4)\n",
    "solver = core.Engine(task, train_set, valid_set, test_set, optimizer,\n",
    "                     gpus=[0], batch_size=4)\n",
    "solver.train(num_epoch=10)\n",
    "solver.evaluate(\"valid\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-07T11:24:01.514413Z",
     "start_time": "2024-02-07T11:23:54.095671Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "注意：我们可以观察到，在AUPRC和F1 max方面，GearNet-Edge在这个小型基准测试中表现优于GearNet。然而，这两个模型的性能都不太令人满意，主要是由于数据集的规模过小。我们建议用户在TorchProtein的标准数据集（例如datasets.EnzymeCommission）上应用这两个模型，以更好地研究它们的有效性。"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
